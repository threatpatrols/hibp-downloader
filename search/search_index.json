{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"hibp-downloader","text":"<p>This is a CLI tool to efficiently download a local copy of the pwned password hash data from the very awesome HIBP pwned passwords api-endpoint using all the good bits; multiprocessing, async-processes, local-caching, content-etags and http2-connection pooling to probably make things  as fast as is Pythonly possible.</p>"},{"location":"#features","title":"Features","text":"<ul> <li>Interface to directly <code>query</code> for compromised password values from the compressed file data-store!</li> <li>Download and store acquired data in gzip'd compressed to save on storage and speed up queries. </li> <li>Download the full dataset in under 45 mins (generally CPU bound)</li> <li>Easily resume interrupted <code>download</code> operations into a <code>--data-path</code> without re-clobbering api-source.</li> <li>Only download hash-prefix content blocks when the source content has changed (via content ETAG values); making it     easy to periodically sync-up when needed.</li> <li>Query interface performance is efficient enough to attach a user web-service with reasonable loads (ie don't waste     your own resources decompressing the dataset and storing in a database!)</li> <li>Ability to generate a single text file with in-order pwned password hash values, similar to PwnedPasswordsDownloader from     the awesome HIBP team.</li> <li>Per prefix file metadata in JSON format for easy data reuse by other tooling if required.</li> </ul>"},{"location":"#install","title":"Install","text":"<pre><code>pipx install hibp-downloader\n</code></pre>"},{"location":"#usage-download","title":"Usage (download)","text":""},{"location":"#performance","title":"Performance","text":"<p>Sample download activity log; host with 32 cores on 500Mbit/s connection.  <pre><code>...\n2024-05-16T10:18:01-0400 | INFO | hibp-downloader | prefix=f80c7 source=[lc:13616 et:3 rc:1002358 ro:25 xx:1] processed=[17836.6MB ~414462H/s] api=[918req/s 17597.4MB] runtime=36.4min\n2024-05-16T10:18:02-0400 | INFO | hibp-downloader | prefix=f81af source=[lc:13616 et:3 rc:1002558 ro:25 xx:1] processed=[17840.1MB ~414454H/s] api=[918req/s 17600.9MB] runtime=36.4min\n2024-05-16T10:18:02-0400 | INFO | hibp-downloader | prefix=f826f source=[lc:13616 et:3 rc:1002758 ro:25 xx:1] processed=[17843.6MB ~414454H/s] api=[918req/s 17604.4MB] runtime=36.4min\n2024-05-16T10:18:03-0400 | INFO | hibp-downloader | prefix=f833f source=[lc:13616 et:3 rc:1002958 ro:25 xx:1] processed=[17847.1MB ~414450H/s] api=[918req/s 17607.9MB] runtime=36.4min\n</code></pre></p> <ul> <li>918x requests per second to <code>api.pwnedpasswords.com</code></li> <li>Log sources are shorthand:<ul> <li><code>lc</code>: 13616 from local-cache (lc) - request-responses handled locally without hitting the network. </li> <li><code>et</code>: 3 etag-matched (et) - request-responses that confirmed our local data was up-to-date and did not require a new download.</li> <li><code>rc</code>: 1002958 from remote-cache (rc) - request-responses that were downloaded to local, but came from the remote-server cache.</li> <li><code>ro</code>: 25 from remote-origin (ro) - request-responses that were downloaded to local, and the download needed to be fetched from remote origin source.</li> <li><code>xx</code>: 1 failed responses - request-responses that failed (and successfully retried).</li> </ul> </li> <li>~17GB downloaded in ~36 minutes (full dataset)</li> <li>Approx ~414k hash values received per second</li> <li>Processing in this example appears to be CPU bound, measured traffic around ~160 Mbit/s.</li> </ul>"},{"location":"#usage-query","title":"Usage (query)","text":""},{"location":"#project","title":"Project","text":"<ul> <li>Docs - threatpatrols.github.io/hibp-downloader</li> <li>PyPI - pypi.org/project/hibp-downloader/</li> <li>Github - github.com/threatpatrols/hibp-downloader</li> </ul>"},{"location":"development/","title":"Development","text":"<p>This project uses the very awesome slap-cli utility to help with  development, testing, packaging and release management.  Slap-cli works well with the fancy-and-fast UV tooling too. </p>"},{"location":"development/#slap-cli","title":"slap-cli","text":"<pre><code># Create a new venv \"env-alias\" to work within\nslap venv -cg env-alias\n\n# Activate the \"env-alias\" venv\nslap venv -ag env-alias\n\n# Install the requirements for the \"env-alias\" development venv\nslap install --upgrade --link\n\n# Update code formatting\nslap run format\n\n# Test the package (pytest, uv)\nslap test\n\n# Write a \"feature\" changelog entry\nslap changelog add -t \"feature\" -d \"&lt;changelog message&gt;\" [--issue &lt;issue_url&gt;]\n\n# Bump the package version at the \"patch\" semver level\nslap release patch --dry\nslap release patch --tag [--push]\n\n# Build a package\nslap publish --build-directory build --dry\n\n# Publish a package\nslap publish\n</code></pre>"},{"location":"license/","title":"License","text":""},{"location":"license/#bsd-3-clause","title":"BSD 3 Clause","text":"<pre><code>Redistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are met:\n\n1. Redistributions of source code must retain the above copyright notice, this\n   list of conditions and the following disclaimer.\n\n2. Redistributions in binary form must reproduce the above copyright notice,\n   this list of conditions and the following disclaimer in the documentation\n   and/or other materials provided with the distribution.\n\n3. Neither the name of the copyright holder nor the names of its\n   contributors may be used to endorse or promote products derived from\n   this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\"\nAND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE\nIMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\nDISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE\nFOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL\nDAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR\nSERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER\nCAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY,\nOR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\nOF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n</code></pre>"},{"location":"license/#copyright","title":"Copyright","text":"<ul> <li>Copyright \u00a9 2023 Nicholas de Jong</li> <li>Copyright \u00a9 2023-2025 Threat Patrols Pty Ltd</li> </ul> <p>All rights reserved.</p>"},{"location":"project/","title":"Project","text":""},{"location":"project/#project-links","title":"Project Links","text":"<ul> <li>Docs - threatpatrols.github.io/hibp-downloader</li> <li>PyPI - pypi.org/project/hibp-downloader/</li> <li>Github - github.com/threatpatrols/hibp-downloader</li> </ul>"},{"location":"project/#features-bugs","title":"Features / Bugs","text":"<p>Please submit all feature-requests and bug-reports via Github issues:-</p> <ul> <li>github.com/threatpatrols/hibp-downloader/issues</li> </ul>"},{"location":"project/#contact","title":"Contact","text":"<ul> <li>Threat Patrols, continuous cybersecurity threat management - www.threatpatrols.com</li> </ul>"},{"location":"command/download/","title":"Download","text":"<p>The <code>hibp-downloader</code> CLI tool attempts to be as fast and efficient as Pythonly possible.</p> <p>The downloader works by creating multiple threads (based on CPU cores) a worker-queue arrangement that each  invoke async-workers on chunks of the hash-prefix address-space.</p> <p>The downloader collects the content for each hash-prefix in gzip format which is a deliberate choice over brotli  compression because CLI tools such as <code>zcat</code> and <code>zgrep</code> work directly with the stored files and similar brotli-based  alternatives are not common (or exist?);  This also means we are able to write the received content directly to disk without any decompression processing.  This saves considerable compute and processing time.</p> <p>The downloader stores a <code>.meta</code> file (JSON, you can also use) alongside each content file that tracks the content timestamps, checksums and ETAG value; it is possible to use the <code>--metadata-path</code> option to store the metadata separately if required.</p> <p>Because the downloader tracks content ETAG values we only receive new content from the remote-source (ie  <code>api.pwnedpasswords.com</code>) when the content has actually changed.  The user is able to override this using  the <code>--ignore-etag</code> option that will force all content to be sent by the source without regard for the ETAG.</p> <p>The downloader also prevents the user from requesting the same hash-prefix content block more than once per  local-cache-ttl to prevent unnecessary re-requests for the same content in short time periods (default 12 hrs); use  the <code>--local-cache-ttl</code> option to adjust this if needed.</p> <p>The <code>--force</code> option is simply a convenience option that sets both <code>--ignore-etag</code> and <code>--local-cache-ttl=0</code> </p> <p>The options <code>--hash-type</code>, <code>--first-hash</code>, <code>--last-hash</code>, <code>--processes</code>, <code>--chunk-size</code>, <code>--http-proxy</code> and <code>--http-certificates</code> are described in the application-help and should be self-evident how to use.</p>"},{"location":"command/download/#usage","title":"Usage","text":""},{"location":"command/download/#logs","title":"Logs","text":"<p>The downloader logs emit information about the download progress, not each requested hash-prefix content  object.  The following attributes are logged -</p> <ul> <li><code>prefix=</code> the current hash prefix content downloaded.</li> <li><code>source.lc=</code> count of locally-cached content objects that did not require any request.</li> <li><code>source.et</code> count of etag-match content objects that did not require re-download from source.</li> <li><code>source.rc</code> count of remote-cached content objects that already existed at the edge-cache provider (ie Cloudflare).</li> <li><code>source.ro</code> count of remote-origin content objects that needed to be retrieved from origin.</li> <li><code>source.xx</code> count of content objects that have unknown remote cache status. </li> </ul>"},{"location":"command/download/#example","title":"Example","text":"<pre><code>$ hibp-downloader --data-path /opt/storage/hibp-datastore download --first-hash \"00000\" --last-hash \"00fff\" --force\n2023-11-12T21:23:57+1000 | INFO | hibp-downloader | HIBP Downloader: v0.1.5\n2023-11-12T21:23:57+1000 | INFO | hibp-downloader | data-path '/opt/storage/hibp-datastore'\n2023-11-12T21:23:57+1000 | INFO | hibp-downloader | metadata-path '/opt/storage/hibp-datastore'\n2023-11-12T21:23:57+1000 | INFO | hibp-downloader | Created 12 worker processes to consume a queue of prefix-hash values.\n2023-11-12T21:23:59+1000 | INFO | hibp-downloader | prefix=00059 source=[lc:0 et:0 rc:50 ro:0 xx:0] processed=[0.8MB ~17290H/s] api=[40req/s 0.8MB] runtime=0.0min\n2023-11-12T21:23:59+1000 | INFO | hibp-downloader | prefix=00077 source=[lc:0 et:0 rc:100 ro:0 xx:0] processed=[1.7MB ~28653H/s] api=[67req/s 1.7MB] runtime=0.0min\n2023-11-12T21:24:01+1000 | INFO | hibp-downloader | prefix=0009f source=[lc:0 et:0 rc:150 ro:0 xx:0] processed=[2.5MB ~27690H/s] api=[66req/s 2.5MB] runtime=0.1min\n2023-11-12T21:24:01+1000 | INFO | hibp-downloader | prefix=000a9 source=[lc:0 et:0 rc:200 ro:0 xx:0] processed=[3.3MB ~32863H/s] api=[78req/s 3.3MB] runtime=0.1min\n2023-11-12T21:24:03+1000 | INFO | hibp-downloader | prefix=000f9 source=[lc:0 et:0 rc:250 ro:0 xx:0] processed=[4.1MB ~32441H/s] api=[77req/s 4.1MB] runtime=0.1min\n</code></pre>"},{"location":"command/generate/","title":"Generate","text":"<p>Allows the user to generate a single decompressed text file with pwned password hash values in-order that is similar  to the single-file output PwnedPasswordsDownloader from the HIBP team.</p>"},{"location":"command/generate/#usage","title":"Usage","text":""},{"location":"command/generate/#example","title":"Example","text":"<pre><code>$ hibp-downloader --data-path /opt/storage/hibp-datastore generate --filename /tmp/onebigfile.txt\n2023-11-12T21:53:31+1000 | INFO | hibp-downloader | HIBP Downloader: v0.1.5\n2023-11-12T21:53:31+1000 | INFO | hibp-downloader | data-path '/opt/storage/hibp-datastore'\n2023-11-12T21:53:31+1000 | INFO | hibp-downloader | Prefix position '00000' appending to '/tmp/onebigfile.txt'\n2023-11-12T21:53:32+1000 | INFO | hibp-downloader | Prefix position '00190' appending to '/tmp/onebigfile.txt'\n2023-11-12T21:53:32+1000 | INFO | hibp-downloader | Prefix position '00320' appending to '/tmp/onebigfile.txt'\n2023-11-12T21:53:33+1000 | INFO | hibp-downloader | Prefix position '004b0' appending to '/tmp/onebigfile.txt'\n</code></pre>"},{"location":"command/query/","title":"Query","text":"<p>The ability to directly query plain-text passwords against the in-place <code>--data-path</code> makes it possible to quickly determine if a matching hash is in the dataset without needing to manually compress or try and store the dataset in a database.</p> <p>The lookup is fast and efficient.</p> <p>This is achieved by -</p> <ul> <li>taking a sha1/ntlm hash of the user supplied password</li> <li>using this to determine the local compressed content-file this password should exist in if it does exist at all </li> <li>decompress the hash-prefix content file and seek the actual hash.</li> </ul> <p>This process is quite efficient in its own right and can support decent-enough queries per second such that you likely do not need to implement a database to have a service query this dataset.  Consider the <code>--quiet</code> option to </p>"},{"location":"command/query/#usage","title":"Usage","text":"<p>CAUTION: by default the CLI will ask the user to input the password (without being shown), it is also possible to use the <code>--password</code> input option to pass this in directly;  If you do this you must understand the potential for those passwords to be recorded in clear-text in shell-histories and perhaps other logs. </p>"},{"location":"command/query/#example-with-prompt-input","title":"Example: with prompt input","text":"<pre><code>$ hibp-downloader --data-path /opt/storage/hibp-datastore query\n2023-11-12T22:03:23+1000 | INFO | hibp-downloader | HIBP Downloader: v0.1.5\nPassword:\n2023-11-12T22:03:26+1000 | INFO | hibp-downloader | data-path '/opt/storage/hibp-datastore'\n{\n  \"data_path\": \"/opt/storage/hibp-datastore\",\n  \"hash\": \"8843D7F92416211DE9EBB963FF4CE28125932878\",\n  \"hash_type\": \"sha1\",\n  \"hibp_count\": 19563,\n  \"status\": \"Found\"\n}\n</code></pre>"},{"location":"command/query/#example-using-password-option-input","title":"Example: using --password option input","text":"<pre><code>$ hibp-downloader --data-path /opt/storage/hibp-datastore query --password foobar\n2023-11-12T22:05:38+1000 | INFO | hibp-downloader | HIBP Downloader: v0.1.5\n2023-11-12T22:05:38+1000 | INFO | hibp-downloader | data-path '/opt/storage/hibp-datastore'\n{\n  \"data_path\": \"/opt/storage/hibp-datastore\",\n  \"hash\": \"8843D7F92416211DE9EBB963FF4CE28125932878\",\n  \"hash_type\": \"sha1\",\n  \"hibp_count\": 19563,\n  \"status\": \"Found\"\n}\n</code></pre>"},{"location":"command/query/#example-query-time-using-password-option","title":"Example: query time using --password option","text":"<p>Response time at around 600ms from NFS backed storage location; fast enough to be used directly for services.  <pre><code>$ time hibp-downloader --quiet --data-path /opt/storage/hibp-datastore query --password foobar\n{\n  \"data_path\": \"/opt/storage/hibp-datastore\",\n  \"hash\": \"8843D7F92416211DE9EBB963FF4CE28125932878\",\n  \"hash_type\": \"sha1\",\n  \"hibp_count\": 19563,\n  \"status\": \"Found\"\n}\n\nreal    0m0.591s\nuser    0m0.446s\nsys     0m0.052s\n</code></pre></p>"}]}